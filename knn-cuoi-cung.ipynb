{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport re\nimport pickle\n\ndir_path = os.path.dirname(os.path.realpath(os.getcwd()))\ndir_path = os.path.join(dir_path, 'Data')\n\ndef get_data(folder_path):\n    X = []\n    y = []\n    dirs = os.listdir(folder_path)\n    for path in dirs:\n        file_paths = os.listdir(os.path.join(folder_path, path))\n        for file_path in file_paths:\n            with open(os.path.join(folder_path, path, file_path), 'r', encoding=\"utf-16\") as f:\n                lines = f.readlines()\n                lines = ' '.join(lines)\n                lines = lines.lower()\n                lines = re.sub(r\"[^\\w\\d\\s]\",\" \",lines,flags=re.UNICODE)\n                lines = re.sub(\"[0-9]\",\" \",lines,flags=re.UNICODE)\n                lines = lines.split()\n                lines = ' '.join(lines)\n                X.append(lines)\n                y.append(path)\n    return X, y\ntrain_path = ('../input/10chudenhoknn/new train - Copy (3)')\nX_data, y_data = get_data(train_path)\npickle.dump(X_data, open('X_data.pkl', 'wb'))\npickle.dump(y_data, open('y_data.pkl', 'wb'))\n\ntest_path = os.path.join('../input/10chudenhoknn/new test - Copy (3)')\nX_test, y_test = get_data(test_path)\n\npickle.dump(X_test, open('X_test.pkl', 'wb'))\npickle.dump(y_test, open('y_test.pkl', 'wb'))\n\nimport pandas as pd\nfrom math import sqrt\nfrom random import randrange\nimport re\n\nX_data = pickle.load(open('X_data.pkl', 'rb'))\ny_data = pickle.load(open('y_data.pkl', 'rb'))\n\nX_test = pickle.load(open('X_test.pkl', 'rb'))\ny_test = pickle.load(open('y_test.pkl', 'rb'))\ndef getNumOfWords(X_data_):\n    numOfWords=[]\n    for bagOfWords in X_data_:\n        numOfWordsTemp = dict.fromkeys(set(bagOfWords.split(' ')).union(), 0)\n        for word in bagOfWords.split(' '):\n            numOfWordsTemp[word] += 1\n        numOfWords.append(numOfWordsTemp)\n    return numOfWords\ndef tfTranform(X_data_):\n    numOfWords = getNumOfWords(X_data_)\n    tf=[]\n    for index, bagOfWords in enumerate(X_data_, start=0):\n        tfTemp = computeTF(numOfWords[index], bagOfWords.split(' '))\n        tf.append(tfTemp)\n    return tf\ndef computeTF(wordDict, bagOfWords):\n    tfDict = {}\n    bagOfWordsCount = len(bagOfWords)\n    for word, count in wordDict.items():\n        tfDict[word] = count / float(bagOfWordsCount)\n    return tfDict\nX_data_tf = tfTranform(X_data)\nX_test_tf = tfTranform(X_test)\npickle.dump(X_test_tf, open('X_data_tf.pkl', 'wb'))\npickle.dump(X_test_tf, open('X_test_tf.pkl', 'wb'))\n# calculate the Euclidean distance between two vectors\ndef euclidean_distance(row1, row2):\n\tdistance = 0.0\n\tfor word in row1:\n\t\tif word in row2:\n\t\t\tdistance += (row1[word] - row2[word])**2\n\t\telse:\n\t\t\tdistance += row1[word]**2\n\tfor word in row2:\n\t\tif word in row1:\n\t\t\tpass\n\t\telse:\n\t\t\tdistance += row2[word]**2\n\treturn sqrt(distance)\n# Locate the most similar neighbors\ndef get_neighbors(train, test_row, num_neighbors):\n\tdistances = list()\n\tfor i in range(len(train)):\n\t\tdist = euclidean_distance(test_row, train[i])\n\t\tdistances.append((i, dist))\n\tdistances.sort(key=lambda tup: tup[1])\n\tneighbors = list()\n\tfor i in range(num_neighbors):\n\t\tneighbors.append(distances[i])\n# \tprint(neighbors)\n\treturn neighbors\n\n# Make a classification prediction with neighbors\ndef predict_classification(train, test_row, num_neighbors):\n\tneighbors = get_neighbors(train, test_row, num_neighbors)\n\tresultTemp = list()\n\tresultTemp_key=[]\n\tfor row,dist in neighbors:\n\t\tif dist == 0.0:\n\t\t\tresultTemp = list()\n\t\t\tresultTemp_key=[]\n\t\t\tresultTemp.append((y_data[row],int(99999)))\n\t\t\tresultTemp_key.append(y_data[row])\n\t\t\tbreak\n\t\telse:\n\t\t\tresultTemp.append((y_data[row],1/dist))\n\t\t\tresultTemp_key.append(y_data[row])\n# \tprint(resultTemp)\n\toutput_values = dict.fromkeys(set(resultTemp_key).union(), 0)\n\tfor label,weight in resultTemp:\n\t\toutput_values[label] += weight\n# \toutput_values = [y_data[row] for row,dist in neighbors]\n# \tprint(output_values)\n\tprediction = max(set(output_values), key=lambda k: output_values[k])\n\treturn prediction\n# test = []\n# with open('../input/smallknntrain/new train - Copy/Am nhac/AN_TN_ (878).txt', 'r', encoding=\"utf-16\") as f:\n# \tlines = f.readlines()\n# \tlines = ' '.join(lines)\n# \tlines = re.sub(r\"[^\\w\\d\\s]\",\" \",lines,flags=re.UNICODE)\n# \tlines = re.sub(\"[0-9]\",\" \",lines,flags=re.UNICODE)\n# \tlines = lines.split()\n# \tlines = ' '.join(lines)\n# \ttest.append(lines)\n\nimport time\ntime_start = time.time()\ndef accuracy_metric(actual, predicted):\n\tcorrect = 0\n\tfor i in range(len(actual)):\n\t\tif actual[i] == predicted[i]:\n\t\t\tcorrect += 1\n\treturn correct / float(len(actual)) * 100.0\npredict_test = []\nfor row in range(len(X_test_tf)):\n    predict_test.append(predict_classification(X_data_tf,X_test_tf[row],3))\nprint(accuracy_metric(y_test,predict_test))\ntime_end = time.time()\nprint(time_end - time_start)","execution_count":1,"outputs":[{"output_type":"stream","text":"68.14715118887393\n718.3579404354095\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(predict_classification(X_data_tf,X_test_tf[1000],3))\nprint(y_test[1000])","execution_count":2,"outputs":[{"output_type":"stream","text":"Bat dong san\nBat dong san\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_test = []\nfor row in range(len(X_test_tf)):\n    predict_test.append(predict_classification(X_data_tf,X_test_tf[row],5))\nprint(accuracy_metric(y_test,predict_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}